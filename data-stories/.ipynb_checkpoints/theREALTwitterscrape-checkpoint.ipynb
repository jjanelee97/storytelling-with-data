{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from selenium import webdriver\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '2dSyaPR8HQv3pal4tpfPaxEIZ'\n",
    "consumer_secret = 'JaB1nVrsc0TYWLOvzVkoM6feHqtBAruKCWn9erz9SJpYu71wNw'\n",
    "access_token = '2952993774-NHXUYXLgs4JKc3NvSTL6cUPwsLKoqgxCxTpKnho'\n",
    "access_secret = 'IOYodaEvwQplWPZboTJPX8cdBNuNux2IfVsdUXjuIkjcn'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "from twitterscraper.query import query_tweets_once as query_tweets_advanced\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "import hypertools as hyp\n",
    "import numpy as np\n",
    "from textblob import TextBlob as tb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for scraping twitter for one or more keywords and returning a dictionary with:\n",
    "# - tweets: the tweet text (list of length n_tweets)\n",
    "# - datetimes: the tweet date/time (as a DateTime object)\n",
    "# - topicvecs: the tweet topic vectors (numpy array with n_tweets rows and n_topics columns)\n",
    "# - topwords: the top n words from each topic (list of length n_topics, where each element is a list of n_words)\n",
    "# - sentiments: the sentiment valence of each tweet (numpy array of length n_tweets)\n",
    "\n",
    "def twitter_witch(keywords, n_tweets=500, n_topics=10, n_words=5, model=None, use_advanced=False):\n",
    "    \n",
    "    #get the tweets\n",
    "    tweets = []\n",
    "    tweets = query_tweets_advanced(keywords, num_tweets=n_tweets, limit=n_tweets)\n",
    "    \n",
    "    #get the tweet text\n",
    "    tweet_text = list(map(lambda x: x.text, tweets))\n",
    "    \n",
    "    return {'tweets': tweet_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(username= \"realDonaldTrump\", n_tweets=100):\n",
    "    \n",
    "    count = 1\n",
    "    twitter_data = []\n",
    "    trump_data = []\n",
    "    \n",
    "    print('Getting information for ')\n",
    "\n",
    "    # https://twitter.com/search? l=en&q=from%3Alilliannzhao&src=typd\n",
    "    # l=&q=from%3ArealDonaldTrump%20since%3A2017-10-03%20until%3A2017-10-10\n",
    "    \n",
    "    while count <= n_tweets:\n",
    "        search_string = 'l=en&q=from%3Alilliannzhao&src=typd'\n",
    "        \n",
    "        if len(twitter_data) == 0:\n",
    "            twitter_data.append(twitter_witch(search_string, n_tweets=n_tweets, use_advanced=True))\n",
    "        # else:\n",
    "            # twitter_data.append(twitter_witch(search_string, n_tweets=n_tweets, use_advanced=True, model=twitter_data[0]['model']))\n",
    "            # print('.', end='') \n",
    "            count = count + 1\n",
    "\n",
    "        print('done')\n",
    "    \n",
    "        return {'tweets': twitter_data, 'trump': trump_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, rate_delay, error_delay=5):\n",
    "        \"\"\"\n",
    "        :param rate_delay: How long to pause between calls to Twitter\n",
    "        :param error_delay: How long to pause when an error occurs\n",
    "        \"\"\"\n",
    "        self.rate_delay = rate_delay\n",
    "        self.error_delay = error_delay\n",
    "\n",
    "    def search(self, query):\n",
    "        self.perform_search(query)\n",
    "\n",
    "    def perform_search(self, query):\n",
    "        \"\"\"\n",
    "        Scrape items from twitter\n",
    "        :param query:   Query to search Twitter with. Takes form of queries constructed with using Twitters\n",
    "                        advanced search: https://twitter.com/search-advanced\n",
    "        \"\"\"\n",
    "        url = self.construct_url(query)\n",
    "        continue_search = True\n",
    "        min_tweet = None\n",
    "        response = self.execute_search(url)\n",
    "        while response is not None and continue_search and response['items_html'] is not None:\n",
    "            tweets = self.parse_tweets(response['items_html'])\n",
    "\n",
    "            # If we have no tweets, then we can break the loop early\n",
    "            if len(tweets) == 0:\n",
    "                break\n",
    "\n",
    "            # If we haven't set our min tweet yet, set it now\n",
    "            if min_tweet is None:\n",
    "                min_tweet = tweets[0]\n",
    "\n",
    "            continue_search = self.save_tweets(tweets)\n",
    "\n",
    "            # Our max tweet is the last tweet in the list\n",
    "            max_tweet = tweets[-1]\n",
    "            if min_tweet['tweet_id'] is not max_tweet['tweet_id']:\n",
    "                if \"min_position\" in response.keys():\n",
    "                    max_position = response['min_position']\n",
    "                else:\n",
    "                    max_position = \"TWEET-%s-%s\" % (max_tweet['tweet_id'], min_tweet['tweet_id'])\n",
    "                url = self.construct_url(query, max_position=max_position)\n",
    "                # Sleep for our rate_delay\n",
    "                sleep(self.rate_delay)\n",
    "                response = self.execute_search(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_search(self, query):\n",
    "\n",
    "        # Scrape items from twitter\n",
    "        # :param query:   Query to search Twitter with. Takes form of queries constructed with using Twitters\n",
    "                        # advanced search: https://twitter.com/search-advanced\n",
    "            \n",
    "        url = self.construct_url(query)\n",
    "        continue_search = True\n",
    "        min_tweet = None\n",
    "        response = self.execute_search(url)\n",
    "        \n",
    "        while response is not None and continue_search and response['items_html'] is not None:\n",
    "            tweets = self.parse_tweets(response['items_html'])\n",
    "\n",
    "            # If we have no tweets, then we can break the loop early\n",
    "            if len(tweets) == 0:\n",
    "                break\n",
    "\n",
    "            # If we haven't set our min tweet yet, set it now\n",
    "            if min_tweet is None:\n",
    "                min_tweet = tweets[0]\n",
    "\n",
    "            continue_search = self.save_tweets(tweets)\n",
    "\n",
    "            # Our max tweet is the last tweet in the list\n",
    "            max_tweet = tweets[-1]\n",
    "            if min_tweet['tweet_id'] is not max_tweet['tweet_id']:\n",
    "                if \"min_position\" in response.keys():\n",
    "                    max_position = response['min_position']\n",
    "                else:\n",
    "                    max_position = \"TWEET-%s-%s\" % (max_tweet['tweet_id'], min_tweet['tweet_id'])\n",
    "                \n",
    "                url = self.construct_url(query, max_position=max_position)\n",
    "                \n",
    "                # Sleep for our rate_delay\n",
    "                sleep(self.rate_delay)\n",
    "                response = self.execute_search(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "perform_search() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-227e6cb86e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperform_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://twitter.com/search?l=en&q=from%3APOTUS&src=typd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: perform_search() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "perform_search(query = \"https://twitter.com/search?l=en&q=from%3APOTUS&src=typd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting information for \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "info = get_tweets(n_tweets = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Muchas gracias amiga, te amo',\n",
       " 'Thank you, Len!',\n",
       " '駅のロータリーの変なとこで止まるやつクソ邪魔\\n後ろ詰まってんじゃん',\n",
       " 'Karakuri Burst by Kagamine Rik and Len',\n",
       " '@sevttaapp sen harbi aptal ve iftiraCı birisin len',\n",
       " '4DA6CCC4 :参戦ID\\n参加者募集！\\nLv60 リヴァイアサン・マグナpic.twitter.com/YTO6sRSpH4',\n",
       " 'Trump',\n",
       " 'i want to cosplay every vananice len that exists please',\n",
       " 'うわーなんで20日休みじゃないんだ、、フォロワーさんが一日店長するのに、、なんで仕事してんだわたし',\n",
       " 'Chocolate con empanadas',\n",
       " 'var i, j, live_neighbours, tmp, x, y, _i, _len, _ref;',\n",
       " 'That is a lot of Limelight.',\n",
       " 'Delleniyiruk len sigoş ',\n",
       " '@DynastyTradesHQ @FFDynastyTrades @DynoTradeCast @DynastyExpert @DynastyTrades @Dynasty_Addict @dynasty_beast @FantasyOutlaw @FFD260',\n",
       " 'اوف ي ابو داحم',\n",
       " '2B87A44E :参戦ID\\n参加者募集！\\nLv70 コロッサス・マグナpic.twitter.com/GS4pDH4KjQ',\n",
       " 'A great win over Shawnee today! 9-4 !! we are so proud of our girls #roadtoyoffs',\n",
       " '!!!!https://twitter.com/peachyksj/status/917868550966202368\\xa0…',\n",
       " \"C'EST AUSSI CONSTATER A QUEL POINT LE VER EST DANS LE FRUIT EN FRANCE ET QU'IL EST PEUT-ÊTRE TROP TARD POUR L'EN EXTRAIRE SANS DOULEUR...\",\n",
       " 'October 20 na kuhaan ng card! feeling ko may bagsak ako ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info['tweets'][0]['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweets': [{'tweets': ['Muchas gracias amiga, te amo', 'Thank you, Len!', '駅のロータリーの変なとこで止まるやつクソ邪魔\\n後ろ詰まってんじゃん', 'Karakuri Burst by Kagamine Rik and Len', '@sevttaapp sen harbi aptal ve iftiraCı birisin len', '4DA6CCC4 :参戦ID\\n参加者募集！\\nLv60 リヴァイアサン・マグナpic.twitter.com/YTO6sRSpH4', 'Trump', 'i want to cosplay every vananice len that exists please', 'うわーなんで20日休みじゃないんだ、、フォロワーさんが一日店長するのに、、なんで仕事してんだわたし', 'Chocolate con empanadas', 'var i, j, live_neighbours, tmp, x, y, _i, _len, _ref;', 'That is a lot of Limelight.', 'Delleniyiruk len sigoş ', '@DynastyTradesHQ @FFDynastyTrades @DynoTradeCast @DynastyExpert @DynastyTrades @Dynasty_Addict @dynasty_beast @FantasyOutlaw @FFD260', 'اوف ي ابو داحم', '2B87A44E :参戦ID\\n参加者募集！\\nLv70 コロッサス・マグナpic.twitter.com/GS4pDH4KjQ', 'A great win over Shawnee today! 9-4 !! we are so proud of our girls #roadtoyoffs', '!!!!https://twitter.com/peachyksj/status/917868550966202368\\xa0…', \"C'EST AUSSI CONSTATER A QUEL POINT LE VER EST DANS LE FRUIT EN FRANCE ET QU'IL EST PEUT-ÊTRE TROP TARD POUR L'EN EXTRAIRE SANS DOULEUR...\", 'October 20 na kuhaan ng card! feeling ko may bagsak ako ']}], 'trump': []}\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
